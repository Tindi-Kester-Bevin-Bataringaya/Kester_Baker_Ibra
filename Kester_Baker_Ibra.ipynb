{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Plan of action\n- Exploratory Data Analysis (EDA)\n- Data cleaning\n- Missingness imputation\n- Encoding categorical variables\n- Outlier removal\n- Feature engineering\n- Scaling\n- Cross-Validation ( Hyperparameter tuning)\n- Modeling\n- Deep Learning Techniques"},{"metadata":{},"cell_type":"markdown","source":"The dataset used in this assignment is described by DeCock at http://jse.amstat.org/v19n3/decock/DataDocumentation.txt"},{"metadata":{},"cell_type":"markdown","source":"Before starting a project, it is important to have an empty environment. This means that no python objects are saved in memory."},{"metadata":{"trusted":false},"cell_type":"code","source":"%whos","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing libraries for manipulating arrays, dataframes and plotting the data. These libraries are Numpy (arrays), matplotlib and seaborn (plotting) as well as pandas (dataframes)"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np # manipulation of arrays\nimport pandas as pd # manipulating dataframes\nimport matplotlib.pyplot as plt # data visualisation\nplt.style.use('ggplot')\nimport seaborn as sns # data visualisation,it is based on plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the data using pandas.read_csv function for comma seperated (.csv) files."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"df_train = pd.read_csv('../input/group-assignment-ace-2020/train.csv')\ndf_test = pd.read_csv('../input/group-assignment-ace-2020/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the best perfomance and also to avoid redundancy in work, the datasets are combined such that the same transformations are applied to both datasets. The last column in the training data is removed since it is the target variable, the transformations are applied to the combined dataset.\n\nThe removed variable is assigned to a new variable (it will be needed later).\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# name of last column in training set\ndf_train.columns[-1] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train=df_train.iloc[:,-1].values\ndf_train.drop([\"SalePrice\"],axis=1,inplace=True)\ndata=pd.concat([df_train,df_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SalePrice column is analysed using the density plot to understand its distribution. The seaborn library has a consice way of doing this."},{"metadata":{"trusted":false},"cell_type":"code","source":"# density plot\nax = sns.distplot(y_train)\nax.set(xlabel = \"Intervals\", ylabel = \"Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SalePrice is not normally distributed. There is right skewedness which means that a small number of houses have a very high price. This suggests log transformation of the variable in order to have a normal distribution."},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy import stats\nfigure = plt.figure(figsize = (13,5))\nplt.subplot(1,2,1)\nstats.probplot(y_train, plot = plt)\nplt.title('Actual SalePrice')\nplt.subplot(1,2,2)\nytrain_log = np.log(y_train)\nstats.probplot(ytrain_log, plot = plt)\nplt.title('SalePrice after log transformation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above is a QQ-plot. For normally distributed data, all values lies along the diagonal across the plot as in the second plot. The few point away from the diagonal are outliers in the data."},{"metadata":{},"cell_type":"markdown","source":"#### Checking the data types of variables in the dataframe <br/>\nThis is important because most algorithms in the scikit-learn expect numerical values. Any non-numeric values are encoded into numeric variables."},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 43 non-numeric variables (object(43)) and 37 numeric variables (float64(11) and int64(26)).\n\nThere are also columns with a lot of missing data such as PoolQC with 10 observed values. This means that most houses do not have a swimming pool. Other variables with a large proportion of missing data are MiscFeature, and Fence."},{"metadata":{},"cell_type":"markdown","source":"#### Checking for proportion of missing data <br/>\nThe impact of missing data on quantitative research can be serious, leading to biased estimates of parameters, loss of information, decreased statistical power, increased standard errors, and weakened generalizability of findings,(Dong & Peng). http://ncbi.nlm.nih.gov/pmc/articles/PMC3701793/"},{"metadata":{"trusted":false},"cell_type":"code","source":"notNA = data.count()/1460  # diving the number of observed non-missing values by the number of rows\n\nfor i in notNA:\n    if i < 0.5: # less than 50% of observed data\n        print(\"Proportion of non-missing data %5.4f at column %d\" % (i, list(notNA).index(i)+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that the 5th, 71th, 72th and 73th columns have more than 50% of the data missing. Rememebr pythonis zero-indexed.\n\nAlternatively, the is.na() finction can be used. This function operates columnwise and returns a boolean dataframe of the same dimensions as the first one. The values can be summed up and the values divided by the number of observations which is the same. as finding the mean. Multiplying the value by 100 returns the percentage of missing values. (isna().sum()/len(df))"},{"metadata":{"trusted":false},"cell_type":"code","source":"missing = data.isna().mean().round(5)*100\n\nfor i in missing:\n    if i > 30: # setting threshold for missing data (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3701793/)\n        print(\"Proportion of missing data:  %5.4f at column %d\" % (i, list(missing).index(i)+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualisation using a bar plot\n# Finding out the columns that are missing values in the dataset\n\nmissing.sort_values(inplace=True)\nmissing.plot.bar(figsize=(15,9), x = 'Features', y = 'Percentage of missing data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features with non-zero values have missing values. However, Fence, Alley, MiscFeature and PoolQC have the largest number of missing values  with PoolQC having the largest value. "},{"metadata":{"trusted":false},"cell_type":"code","source":"data.columns[[6,57,72,73,74]] #python is zero-indexed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns above will be removed/dropped. The criterion for dropping a feature was \"Any feature with more than 30% of the data as missing\". It is stringent but makes analysis easier as well as more accurate predictions since a large number of values are not imputed."},{"metadata":{"trusted":false},"cell_type":"code","source":"miss_features = missing.loc[missing>30].index\nlist(miss_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.drop(miss_features,inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding categorical or non-numeric variables\n\nEncoding methods available in python are; **One Hot Encoding, Label Encoding, Ordinal Encoding,Helmert Encoding, Binary Encoding, Frequency Encoding, Mean Encoding, Weight of Evidence Encoding, Probability Ratio Encoding, Hashing Encoding, Backward Difference Encoding, Leave One Out Encoding, James-Stein Encoding, M-estimator Encoding**\n\nhttps://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\n\nhttps://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\n\nhttps://osf.io/356ed/download (A Benchmark Experiment on How to Encode Categorical Features in Predictive Modeling, Florian Pargent)\n\nIt is necessary to tell apart nominal and ordinal variables as they may require different encoding."},{"metadata":{"trusted":false},"cell_type":"code","source":"#getting a list of non-numeric columns\n\nnum_cols = list(data._get_numeric_data().columns) # getting numeric columns\ncols = list(data.columns) # all the columns\n\ncat_cols = [item for item in cols if item not in num_cols] # use set difference list(set(cols) - set(num_cols))\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# identifying nominal, binary and ordinal variables using the dataset description\ncategorical = pd.DataFrame()\n                           \nfor var in cat_cols:\n    temp = [var, data[var].nunique(), (data[var].unique())]\n    categorical = categorical.append(pd.Series(temp), ignore_index=True)\n\ncategorical.columns = ['feature','number of unique features','unique features']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"categorical\ncategorical.style.set_properties(subset=['unique features'], **{'width': '300px'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nominal features (cannot be ordered) - LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle,RoofStyle, RoofMatl, Exterior1st, Exterior2nd, Foundation, Heating, GarageType, MSZoning, LandContour, Street, CentralAir, SaleType, SaleCondition, MassVnrType\n\nMSSubType is already encoded\n\nPossible encoders - **Target encoding**\n\nOrdinal variables (can be ordered) - LotShape, Utilities, LandSlope, ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, HeatingQC, KitchenQual, Functional, FirePlaceQu, GarageFinish, GarageQual, GarageCond, BsmtFinType1, BsmtType2, Electrical, PavedDrive\n\nPossible encoding - **Ordinal encoding**\n\nOverallCond and OverallQual are ordinal variables but are already encoded"},{"metadata":{"trusted":false},"cell_type":"code","source":"nominal = ['LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', \n           'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n           'Foundation', 'Heating', 'GarageType', 'MSZoning', 'LandContour', \n           'Street', 'CentralAir', 'SaleType', 'SaleCondition', 'MasVnrType']\n\nordinal = ['LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', \n           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'HeatingQC', 'KitchenQual', \n           'Functional', 'GarageFinish', 'GarageQual', 'GarageCond', \n           'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'PavedDrive']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before encoding the non-numerical variables, it is important to identify features with high cardinality (those exhibiting a large number of levels). These have an non-productive effect in the case that decision trees are used in supervised learning. The number of splits done increases the computing power which may not be available."},{"metadata":{"trusted":false},"cell_type":"code","source":"# checking for cardinality (feature with a high number of levels)\nsum(categorical['number of unique features']>=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# seperating categorical features\ncatColumns = data.select_dtypes(include=['object']).copy()\n# print(catColumns.isnull().values.sum())\nprint(catColumns.isnull().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import category_encoders as ce\nfrom sklearn.preprocessing import LabelEncoder\n\n# for nomimal variables  https://medium.com/analytics-vidhya/types-of-categorical-data-encoding-schemes-a5bbeb4ba02b\n\n# create an object of the OrdinalEncoding\nce_ordinal = ce.OrdinalEncoder(cols=nominal)\n# fit and transform and you will get the encoded data\ncatColumns = ce_ordinal.fit_transform(catColumns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" For ordinal variables, ordinal order  was assigned through through dictionaries.\n Variables considered are; LandSlope, ExtrQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, HeatingQC, KitchenQual, Functional, GarageFinish, GarageQual, GarageCond, SaleType. SaleCondition"},{"metadata":{"trusted":false},"cell_type":"code","source":"LandSlope_dict = {'Gtl': 1,\n                      'Med': 2,\n                      'Sev': 3}\ncatColumns['LandSlope'] = catColumns.LandSlope.map(LandSlope_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ExterQual_dict = {'Gd': 3,\n                      'TA': 2,\n                      'Ex': 4,\n                     'Fa':1\n                    }\ncatColumns['ExterQual'] = catColumns.ExterQual.map(ExterQual_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LotShape_dict = {'Reg': 4,\n                      'IR1': 3,\n                      'IR2': 2,\n                     'IR3':1\n                    }\ncatColumns['LotShape'] = catColumns.LotShape.map(LotShape_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Utilities_dict = {'AllPub': 4,\n                      'NoSewr': 3,\n                      'NoSewa': 2,\n                     'ELO':1\n                    }\ncatColumns['Utilities'] = catColumns.Utilities.map(Utilities_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ExterQual_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1\n                    }\ncatColumns['ExterQual'] = catColumns.ExterQual.map(ExterQual_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ExterCond_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1\n                    }\ncatColumns['ExterCond'] = catColumns.ExterCond.map(ExterCond_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BsmtQual_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1,\n                  'NA':0\n                    }\ncatColumns['BsmtQual'] = catColumns.BsmtQual.map(BsmtQual_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BsmtCond_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1,\n                  'NA':0\n                    }\ncatColumns['BsmtCond'] = catColumns.BsmtCond.map(BsmtCond_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BsmtExposure_dict = {'Gd': 4,\n                      'Av': 3,\n                      'Mn': 2,\n                     'No':1,\n                  'NA':0\n                    }\ncatColumns['BsmtExposure'] = catColumns.BsmtExposure.map(BsmtExposure_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BsmtFinType1_dict = {'GLQ':6,\n                 'ALQ':5,\n                  'BLQ': 4,\n                      'Rec': 3,\n                      'LwQ': 2,\n                     'Unf':1,\n                  'NA':0\n                    }\ncatColumns['BsmtFinType1'] = catColumns.BsmtFinType1.map(BsmtFinType1_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"BsmtFinType2_dict = {'GLQ':6,\n                 'ALQ':5,\n                  'BLQ': 4,\n                      'Rec': 3,\n                      'LwQ': 2,\n                     'Unf':1,\n                  'NA':0\n                    }\ncatColumns['BsmtFinType2'] = catColumns.BsmtFinType2.map(BsmtFinType2_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"HeatingQC_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1\n                    }\ncatColumns['HeatingQC'] = catColumns.HeatingQC.map(HeatingQC_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Electrical_dict = {'SBrkr':5,\n                  'FuseA': 4,\n                      'FuseF': 3,\n                      'FuseP': 2,\n                     'Mix':1\n                    }\ncatColumns['Electrical'] = catColumns.Electrical.map(Electrical_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"KitchenQual_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1\n                    }\ncatColumns['KitchenQual'] = catColumns.KitchenQual.map(KitchenQual_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Functional_dict = {'Typ':7,\n                   'Min1':6,\n                 'Min2':5,\n                  'Mod': 4,\n                      'Maj1': 3,\n                      'Maj2': 2,\n                     'Sev':1,\n                  'Sal':0\n                    }\ncatColumns['Functional'] = catColumns.Functional.map(Functional_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"GarageFinish_dict = {'Fin': 3,\n                      'RFn': 2,\n                      'Unf': 1,\n                     'NA':0\n                    }\ncatColumns['GarageFinish'] = catColumns.GarageFinish.map(GarageFinish_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"GarageQual_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1,\n                   'NA':0\n                    }\ncatColumns['GarageQual'] = catColumns.GarageQual.map(GarageQual_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"GarageCond_dict = {'Ex':5,\n                  'Gd': 4,\n                      'TA': 3,\n                      'Fa': 2,\n                     'Po':1,\n                   'NA':0\n                    }\ncatColumns['GarageCond'] = catColumns.GarageCond.map(GarageCond_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"PavedDrive_dict = {'Y': 3,\n                      'P': 2,\n                     'N':1\n                    }\ncatColumns['PavedDrive'] = catColumns.PavedDrive.map(PavedDrive_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the number of missing values in the categorical variables is not large as per the criteria, missing values were imputed using the mode of the category."},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in list(catColumns.columns):\n    if catColumns[var].isnull().mean() > 0:\n        catColumns = catColumns.fillna(catColumns[var].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(catColumns.isnull().mean() > 0) # no more missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling numeric columns\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"numColumns = data._get_numeric_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Proportion of missing data in numeric columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"numColumns.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LotFrontage and GarageYrBlt have the highest number of missing values. \n\nLotFrontage, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF and GarageArea will be imputed using the median or mean\n\nGarageCars, BsmtFullBath, BsmtHalfBath will be imputed from most commonly occuring value\n\nFor missing values in GarageYrBuilt, these indicate absence of a garage and will be set to 0\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"numColumns.GarageYrBlt.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# filling with most commonly occuring value\n\ndiscrete = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']\nfor var in discrete:\n    if numColumns[var].isnull().mean() > 0:\n        numColumns = numColumns.fillna(numColumns[var].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# filling with the median\nnumColumns.LotFrontage = data.groupby('Neighborhood')['LotFrontage'].\\\n                    transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"numColumns.MasVnrArea = data.groupby('Neighborhood')['MasVnrArea'].\\\n                            transform(lambda x: x.fillna(x.median()))\n\nnumColumns.BsmtFinSF1 = data.groupby('Neighborhood')['BsmtFinSF1'].\\\n                            transform(lambda x: x.fillna(x.median()))\n\nnumColumns.BsmtFinSF2 = data.groupby('Neighborhood')['BsmtFinSF2'].\\\n                            transform(lambda x: x.fillna(x.median()))\n\nnumColumns.BsmtUnfSF = data.groupby('Neighborhood')['BsmtUnfSF'].\\\n                            transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"numColumns.isna().mean() # more missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis on numeric columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"# looking at time-related variables (YearBuilt, YearRemodAdd, GarageYrBlt, MoSold, YrSold)\n\n# When are houses sold?\ndata.groupby(['YrSold','MoSold']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.xlabel(\"Year & Month\")\nplt.ylabel(\"Number of houses\")\nplt.title('When are houses sold?')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seasonal pattern for house sales. House sales peak mid-year each year."},{"metadata":{"trusted":false},"cell_type":"code","source":"# When were garages built?\ndata.groupby(['GarageYrBlt']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.ylabel(\"Number of garages built\")\nplt.xlabel(\"Year\")\nplt.title('When garages were built?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Garages became more common in the 2000's"},{"metadata":{"trusted":false},"cell_type":"code","source":"# When houses were built\ndata.groupby(['YearBuilt']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.title('When houses were built')\nplt.ylabel(\"Number of houses built\")\nplt.xlabel(\"Year\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# When houses were remodelled\ndata.groupby(['YearRemodAdd']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.title('When houses were remodeled')\nplt.ylabel(\"Number of houses\")\nplt.xlabel(\"Year of remodelling\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most houses remodelled in the 50's"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Where are the houses?\ndata.groupby('Neighborhood').Id.count().\\\n    sort_values().\\\n    plot(kind='barh', figsize=(6,6))\nplt.title('What neighborhoods are houses in?')\nplt.xlabel(\"Number of houses\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating new features"},{"metadata":{},"cell_type":"markdown","source":"All are numeric. MoSold will be factorised since the euclidean distance between the numbers doesn't offer a lot of information\n\nDifference between YearBuilt and YrSold is the age of the house when it was sold - HouseAge"},{"metadata":{"trusted":false},"cell_type":"code","source":"# concatenating numColumns and catColumns\ndataFull = pd.concat([numColumns, catColumns], axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# checking that dataFull meets the expected dimensions and no missing values\ndataFull.shape\ndataFull.isna().sum().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataFull.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar pattern to building of garages. More houses built with garages"},{"metadata":{},"cell_type":"markdown","source":"Related features that can be summed up\n\nTotalBath = BsmtFullBath + BsmtHalfBath + FullBath + HalfBAth\n\nTotalFlrSF = 1stFlrSF + 2ndFlrSF + LowQualFinSF\n\nTotalBsmtSF is assumed to relate BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF \n\nTotalRooms - TotRmsAbvGrd + TotalBath\n\nHouseSF = GarageArea + WoodDeckSF + TotalBsmtSF + GrLivArea\n\nTotalPorchSF = OpenPorchSF + EnclosedPorch + 3-SsnPorch + ScreenPorch\n\nHouseAgeYr = YrSold - YearBuilt "},{"metadata":{"trusted":false},"cell_type":"code","source":"numNew = ['TotalBath', 'TotalFlrSF', 'TotalBsmtSF', 'TotalRooms', 'HouseSF', 'TotalPorchSF', 'HouseAgeYr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#total number of baths\ndataFull['TotalBath'] = dataFull['BsmtFullBath'] + (dataFull['BsmtHalfBath']*0.5) \\\n                            + dataFull['FullBath'] + (dataFull['HalfBath']*0.5)\n\n#total floor square feet\ndataFull['TotalFlrSF'] = dataFull['1stFlrSF'] + dataFull['2ndFlrSF'] + dataFull['LowQualFinSF']\n\n#total number of rooms\ndataFull['TotalRooms'] =  dataFull['TotRmsAbvGrd'] + dataFull['TotalBath']\n\n#total size of the house in SF\ndataFull['HouseSF'] = dataFull['GarageArea'] + dataFull['WoodDeckSF'] + dataFull['TotalBsmtSF']\\\n                    + dataFull['GrLivArea']\n\n#total size of porches\ndataFull['TotalPorchSF'] = dataFull['OpenPorchSF'] + dataFull['EnclosedPorch'] +\\\n                            dataFull['3SsnPorch'] + dataFull['ScreenPorch']\n\n#age of the house\ndataFull['HouseAgeYr'] = dataFull['YrSold'] - dataFull['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also creating features for factors that are taken into consideration during purchase of a house such as;\n- is the house new or remodeled\n- does it have a garage\n- does it have a basement\n- does it have a porch\n- does it have a pool"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataFull['HasBasement'] = dataFull.TotalBsmtSF.apply(lambda x: 1 if x > 0 else 0)\ndataFull['HasGarage'] = dataFull.GarageArea.apply(lambda x: 1 if x > 0 else 0)\ndataFull['HasPorch'] = dataFull.TotalPorchSF.apply(lambda x: 1 if x > 0 else 0)\ndataFull['HasPool'] = dataFull.PoolArea.apply(lambda x: 1 if x > 0 else 0)\ndataFull['WasRemodeled'] = (dataFull.YearRemodAdd != dataFull.YearBuilt).astype(np.int64)\ndataFull['IsNew'] = (dataFull.YearBuilt > 2000).astype(np.int64)\ndataFull['WasCompleted'] = (dataFull.SaleCondition != 'Partial').astype(np.int64)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"catNew = ['HasBasement','HasGarage', 'HasPorch', 'HasPool', 'WasRemodeled', 'IsNew', 'WasCompleted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# resplitting the data\ndF_train, dF_test = dataFull[0:1460], dataFull[1460:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# joining dF_train to the target variable\ndF_train = pd.concat([dF_train, pd.Series(y_train)], axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dF_train.rename(columns={0:'SalePrice'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More exploratory analysis to understand distribution of the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"dF_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pearsoncorr = dF_train.corr(method='pearson')\n\n# visualizing the correlation matrix as a heatmap\nplt.figure(figsize=(60,60))\ntop_corr = pearsoncorr.index\nsns.heatmap(pearsoncorr, \n            xticklabels=pearsoncorr.columns,\n            yticklabels=pearsoncorr.columns,\n            cmap='RdYlGn',\n            annot=False,\n            linewidth=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There features that are highly correlated with each other as well as with the target variable. (multicollinearity)\n\nAlso, there are features that are highly correlated with SalePrice. These are GrLivArea, OverallQual, TotalBsmtSF, 1stFlrSF, the new features (TotalBath, Total1stFlrSF, TotalRooms and HouseSF), as well as HouseAgeYr (highest negative correlation).\n\nMedium correlated features are LotFrontage,LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Feature sorted by correlation to SalePrice\n\ncorr = pearsoncorr.sort_values('SalePrice', ascending=False)\nplt.figure(figsize=(30,30))\nsns.barplot( corr.SalePrice[1:], corr.index[1:], orient='h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generated features HouseAgeYr and HouseSF have the highest negative and positive correlation respectively."},{"metadata":{},"cell_type":"markdown","source":"Categorical and numerical variables are treated seperately"},{"metadata":{},"cell_type":"markdown","source":"#### Categorical and discrete features"},{"metadata":{"trusted":false},"cell_type":"code","source":"discrete = ['MoSold','YearBuilt','YearRemodAdd','YrSold','GarageYrBlt',\n                                           'TotalBath', 'Fireplaces', 'BsmtFullBath', 'BsmtHalfBath',\n                                           'FullBath', 'HalfBath', 'TotalRooms', 'MSSubClass','GarageCars',\n                                           'TotRmsAbvGrd','BedroomAbvGr', 'HouseAgeYr', 'OverallCond','KitchenAbvGr']\ncatFeatures = catNew + nominal + ordinal + discrete\nnumFeatures = num_cols + numNew","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"numFeatures = list(set(numFeatures) - set(discrete))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"total = catFeatures + numFeatures\ndf = list(dF_train.columns)\n\nlist(set(df) - set(total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Count plots of categorical & discrete features\n\nf = pd.melt(dF_train, value_vars=sorted(catFeatures)) # similar to stack() function in R\ng = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\nplt.xticks(rotation='vertical')\ng = g.map(sns.countplot, 'value')\n[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\ng.fig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Count plots of categorical features\nf = pd.melt(dF_train, id_vars=['SalePrice'], value_vars=sorted(catFeatures))\ng = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False, size=4)\ng = g.map(sns.boxplot, 'value', 'SalePrice')\n[plt.setp(ax.get_xticklabels(), rotation=90) for ax in g.axes.flat]\ng.fig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the box plots, it can be seen that variables have outliers on the upper end of the SalePrice above 700,000."},{"metadata":{"trusted":false},"cell_type":"code","source":"# which categorical feature contributes the most to predicting the SalePrice\nimport scipy.stats\n\nanova = {'feature':[], 'f':[], 'p':[]}\nfor cat in catFeatures:\n    group_prices = []\n    for group in dF_train[cat].unique():\n        group_prices.append(dF_train[dF_train[cat] == group]['SalePrice'].values)\n    f, p = scipy.stats.f_oneway(*group_prices)\n    anova['feature'].append(cat)\n    anova['f'].append(f)\n    anova['p'].append(p)\nanova = pd.DataFrame(anova)\nanova = anova[['feature','f','p']]\nanova.sort_values('p', inplace=True)\n\n# Plot\nplt.figure(figsize=(14,6))\nsns.barplot(anova.feature, np.log(1./anova['p']))\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Continuous features"},{"metadata":{"trusted":false},"cell_type":"code","source":"numFeatures = list(set(numFeatures) - set(['Id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid of distribution plots of all numerical features\n# f = pd.melt(dF_train, value_vars=sorted(numFeatures))\n# g = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\n# g = g.map(sns.distplot, 'value')\n\ndF_train[numFeatures].plot(kind='density', subplots=True, layout=(6,4), figsize=(30,30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Scatter plots of numerical features against SalePrice\nf = pd.melt(dF_train, id_vars=['SalePrice'], value_vars=sorted(numFeatures))\ng = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\nplt.xticks(rotation='vertical')\ng = g.map(sns.regplot, 'value', 'SalePrice', scatter_kws={'alpha':0.3})\n[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\ng.fig.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature transformations\n\nLog transformation of right skewed features\n\nRescaling"},{"metadata":{},"cell_type":"markdown","source":"## Multivariate regression models <br/>\nSince the number of features is very large, regression models wih regularisation will be used to avoid overfitting and making complex models.\n\nModels to be used:\n- Ridge regression\n- LASSO regression\n\nThese will be explored later:\n- Kernel ridge regression\n- Stochastic gradient descent\n- ElasticNet"},{"metadata":{"trusted":false},"cell_type":"code","source":"# dummy variables for nominal and categorical variables\n\n# cat_dum = ['MSSubClass','OverallQual',\n#        'OverallCond','MSZoning', 'Street', 'LotShape',\n#        'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n#        'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n#        'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual',\n#        'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n#        'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir',\n#        'Electrical', 'KitchenQual', 'Functional', 'GarageType', 'GarageFinish',\n#        'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n\nmodel_data = pd.get_dummies(dataFull)\n\n# resplitting\nmodel_train, model_test = model_data[0:1460], model_data[1460:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# remove anything over 4,000 sq ft og GrLivArea\n# model_train.drop(dF_train[dF_train.GrLivArea >= 4000].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# scaling numerical features\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nmodel_train.loc[:,numFeatures] = scaler.fit_transform(model_train[numFeatures])\nmodel_test.loc[:,numFeatures] = scaler.fit_transform(model_test[numFeatures])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# splitting data into model_train into training and validation sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = \\\n                    train_test_split(model_train, y_train, \n                                     test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear regression with cross validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# multiple linear regression - least squares fitting\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict\nfrom sklearn.linear_model import LinearRegression\nkfold = KFold(n_splits = 10, random_state = 42)\nlm = LinearRegression()\n\n# fitting the model\nlm.fit(model_train, ytrain_log)\nmse = cross_val_score(lm, model_train, y_train, scoring=\"neg_mean_squared_error\",\n                     cv=10)\nmean_mse = np.mean(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n# Make cross validated predictions\nlm_predictions = cross_val_predict(lm, model_train, y_train, cv=10)\n\nplt.scatter(y_train, lm_predictions)\n\n\naccuracy = metrics.r2_score(y_train, lm_predictions)\nprint('Cross-Predicted Accuracy:', accuracy)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predicting on new data and writing predictions to a file\ny_predictions = lm.predict(model_test)\nypred_df = pd.concat([df_test.Id, pd.Series(y_predictions)], axis=1)\nypred_df.columns = ['Id','SalePrice']\n\nypred_df.to_csv('2d0.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge regression with regularisation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# ridge regression with cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nridge = Ridge()\n\nparameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n\nridge_regressor = GridSearchCV(ridge, parameters, scoring=\"neg_mean_squared_error\",\n                              cv=10)\n\nridge_regressor.fit(model_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(ridge_regressor.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(ridge_regressor.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make cross validated predictions\nr_predictions = ridge_regressor.predict(model_train,)\n\nplt.scatter(y_train, r_predictions)\n\n\naccuracy = metrics.r2_score(y_train, r_predictions)\nprint('Cross-Predicted Accuracy:', accuracy)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predicting on new data and writing predictions to a file\nridge_predictions = ridge_regressor.predict(model_test)\n\nridgepred_df = pd.concat([df_test.Id, pd.Series(ridge_predictions)], axis=1)\nridgepred_df.columns = ['Id','SalePrice']\n\nridgepred_df.to_csv('2d1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"# lasso regression\nfrom sklearn.linear_model import Lasso\nlasso = Lasso()\nparameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring=\"neg_mean_squared_error\",\n                              cv=10)\n\n\nlasso_regressor.fit(model_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(lasso_regressor.best_score_)\nprint(lasso_regressor.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make cross validated predictions\nl_predictions = lasso_regressor.predict(model_train)\n\nplt.scatter(y_train, l_predictions)\n\n\naccuracy = metrics.r2_score(y_train, l_predictions)\nprint('Cross-Predicted Accuracy:', accuracy)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#predicting on new data and writing predictions to a file\nlasso_predictions = lasso_regressor.predict(model_test)\n\nlassopred_df = pd.concat([df_test.Id, pd.Series(lasso_predictions)], axis=1)\nlassopred_df.columns = ['Id','SalePrice']\n\nlassopred_df.to_csv('2d2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Kernel ridge regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.kernel_ridge import KernelRidge\n\nkernel = KernelRidge()\nparameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n\nkernel_regressor = GridSearchCV(kernel, parameters, scoring=\"neg_mean_squared_error\",\n                              cv=10)\n\n\nkernel_regressor.fit(model_train, y_train)\n\n#predicting on new data and writing predictions to a file\nkernel_predictions = kernel_regressor.predict(model_test)\n\nkernelpred_df = pd.concat([df_test.Id, pd.Series(kernel_predictions)], axis=1)\nkernelpred_df.columns = ['Id','SalePrice']\n\nkernelpred_df.to_csv('2d3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make cross validated predictions\nkr_predictions = kernel_regressor.predict(model_train,)\n\nplt.scatter(y_train, kr_predictions)\n\n\naccuracy = metrics.r2_score(y_train, kr_predictions)\nprint('Cross-Predicted Accuracy:', accuracy)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stochastic Gradient Descent Regressor\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(max_iter=1000, tol=1e-3)\n\n# fitting the model\nkfold = KFold(n_splits = 10, random_state = 42)\nsgd.fit(model_train, ytrain_log)\nmse = cross_val_score(sgd, model_train, y_train, scoring=\"neg_mean_squared_error\",\n                     cv=10)\nmean_mse = np.mean(mse)\n\n\n#predicting on new data and writing predictions to a file\nsgd_predictions = sgd.predict(model_test)\nsgdpred_df = pd.concat([pd.DataFrame(df_test.Id), \n                        pd.DataFrame(sgd_predictions)], axis=1)\nsgdpred_df.columns = ['Id','SalePrice']\n\nsgdpred_df.to_csv('2d4.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make cross validated predictions\nsgd_predictions = cross_val_predict(sgd, model_train, y_train, cv=10)\n\nplt.scatter(y_train, sgd_predictions)\n\n\naccuracy = metrics.r2_score(y_train, sgd_predictions)\nprint('Cross-Predicted Accuracy:', accuracy)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNet with cross validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\nelastic = ElasticNetCV(cv=10, random_state=42)\n\nelastic.fit(model_train, y_train)\n\n#predicting on new data and writing predictions to a file\nelastic_predictions = elastic.predict(model_test)\nelasticpred_df = pd.concat([pd.DataFrame(df_test.Id), \n                            pd.DataFrame(elastic_predictions)], axis=1)\nelasticpred_df.columns = ['Id','SalePrice']\n\n#elasticpred_df.set_index(elasticpred_df.iloc[:,0])\n\nelasticpred_df.to_csv('2d5.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make cross validated predictions\nel_predictions = elastic.predict(model_train)\n\nplt.scatter(y_train, el_predictions)\n\n\naccuracy = metrics.r2_score(y_train, el_predictions)\nprint('Cross-Predicted Accuracy:', accuracy)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep learning techinques"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use some functions from tensorflow_docs\n!pip install -q git+https://github.com/tensorflow/docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_docs as tfdocs\nimport tensorflow_docs.plots\nimport tensorflow_docs.modeling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# # instantiating the model in the strategy scope creates the model on the TPU\n\nwith tpu_strategy.scope():\n    \n    model = tf.keras.Sequential([\n    layers.Dense(32, activation='relu', input_shape=(88,), kernel_initializer='normal'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation = 'linear')])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n    \n    \n    # for mean squared error regression problem\n    model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# training the model\nEPOCHS = 10\nhistory = model.fit(X_train, Y_train,\n  epochs=EPOCHS, validation_split = 0.2, verbose=1,\n  callbacks=[tfdocs.modeling.EpochDots()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualising training process\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\nplotter.plot({'Basic': history}, metric = \"mae\")\nplt.ylim([0, 10])\nplt.ylabel('Sale price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotter.plot({'Basic': history}, metric = \"mse\")\nplt.ylim([0, 20])\nplt.ylabel('MSE [sale price^2]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# making predictions\nY_predictions = model.predict(X_test).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(Y_test, Y_predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nlims = [0, 50]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# error distribution - expect normal distribution\nerror = Y_predictions - Y_test\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error [sale price]\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}